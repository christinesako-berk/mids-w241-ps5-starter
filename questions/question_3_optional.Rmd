# **Optional** Online advertising natural experiment. 

```{problem_description}
These are simulated data (closely, although not entirely) based on a real example, adopted from Randall Lewis’ dissertation at MIT.

Problem Setup 

Imagine Yahoo! sells homepage ads to advertisers that are quasi-randomly assigned by whether the user loads the Yahoo! homepage (www.yahoo.com) on an even or odd second of the day. More specifically, the setup is as follows. On any given week, Monday through Sunday, two ad campaigns are running on Yahoo!’s homepage. If a user goes to www.yahoo.com during an even second that week (e.g., Monday at 12:30:58pm), the ads for the advertiser are shown. But if the user goes to www.yahoo.com during an odd second during that week (e.g., Monday at 12:30:59), the ads for other products are shown. (If a user logs onto Yahoo! once on an even second and once on an odd second, they are shown the first of the campaigns the first time and the second of the campaigns the second time. Assignment is not persistent within users.)

This natural experiment allows us to use the users who log onto Yahoo! during odd seconds/the ad impressions from odd seconds as a randomized control group for users who log onto Yahoo! during even seconds/the ad impressions from even seconds. (We will assume throughout the problem there is no effect of viewing advertiser 2’s ads, from odd seconds, on purchases for advertiser 1, the product advertised on even seconds.)

Imagine you are an advertiser who has purchased advertising from Yahoo! that is subject to this randomization on two occasions. Here is a link to (fake) data on 500,000 randomly selected users who visited Yahoo!’s homepage during each of your two advertising campaigns, one you conducted for product A in March and one you conducted for product B in August (~250,000 users for each of the two experiments). Each row in the dataset corresponds to a user exposed to one of these campaigns.
```

```{r load yahoo data, message=FALSE}
library(data.table)
d <- fread('../data/yahoo_data.csv')
```

```{problem_description}
The variables in the dataset are described below:

  - **product_b**: an indicator for whether the data is from your campaign for product A (in which case it is set to 0), sold beginning on March 1, or for product B, sold beginning on August 1 (in which case it is set to 1). That is, there are two experiments in this dataset, and this variable tells you which experiment the data belong to.
  - **treatment_ad_exposures_week1**: number of ad exposures for the product being advertised during the campaign. (One can also think of this variable as “number of times each user visited Yahoo! homepage on an even second during the week of the campaign.”)
  - **total_ad_exposures_week1**: number of ad exposures on the Yahoo! homepage each user had during the ad campaign, which is the sum of exposures to the “treatment ads” for the product being advertised (delivered on even seconds) and exposures to the “control ads” for unrelated products (delivered on odd seconds). (One can also think of this variable as “total number of times each user visited the Yahoo! homepage during the week of the campaign.”)
  - **week0**: For the treatment product, the revenues from each user in the week prior to the launch of the advertising campaign.
  - **week1**: For the treatment product, the revenues from each user in the week during the advertising campaign. The ad campaign ends on the last day of week 1.
  - **week2-week10**: Revenue from each user for the treatment product sold in the weeks subsequent to the campaign. The ad campaign was not active during this time.
  
Simplifying assumptions you should make when answering this problem:

  - The effect of treatment ad exposures on purchases is linear. That is, the first exposure has the same effect as the second exposure, has the same effect as the third exposure. 
  - There is no effect of being exposed to the odd-second ads on purchases for the product being advertised on the even second.
  - Every Yahoo! user visits the Yahoo! home page at most six times a week.
  - You can assume that treatment ad exposures do not cause changes in future ad exposures.  That is, assume that getting a treatment ad at 9:00am doesn’t cause you to be more (or less) likely to visit the Yahoo home pages on an even second that afternoon , or on subsequent days. This is a setup we need for these estimators to work, but, it might or might not hold in real life. 

First things first, these variable names are frustrating. Rename them: 
```

```{r set name to something sane}
setnames(
  d, 
  old = c('total_ad_exposures_week1', 'treatment_ad_exposures_week1'), 
  new = c('total_ads', 'treatment_ads')
)
```

```{r}
head(d)
```

## Cross table of total_ads and treatment_ads
A. Run a crosstab -- which in R is `table` -- of `total_ads` and `treatment_ads` to sanity check that the distribution of impressions looks as it should. After you write your code, write a few narrative sentences about whether this distribution looks reasonable. Why does it look like this? (No computation required here, just a brief verbal response.)

```{r cross tab, include=TRUE}
# Creating the cross tab
cross_tab <- table(d$total_ads, d$treatment_ads)
cross_tab
```

**Answer:** The table has an upper-triangular structure that reflects the fact that the number of treatment ads never exceeds the total number of homepage visits. Users tend to have treatment exposures distributed roughly symmetrically around half of their total visits, following a binomial pattern that reflects the quasi-random (Bernoulli) assignment of ads to even-second homepage visits.

## Placebo test
A colleague of yours proposes to estimate the following model: `d[ , lm(week1 ~ tretment_ads)]` You are suspicious. Run a placebo test with `week0` purchases as the outcome and report the results. Since treatment is applied in week 1, and `week0` is purchases in week 0, *should* there be an relationship? Did the placebo test “succeed” or “fail”? Why do you say so?

```{r colleague model, include=TRUE}
# Creating placebo regression
model_colleague <- lm(week0 ~ treatment_ads, data = d)
summary(model_colleague)
```

**Answer:** The placebo regression of `week0` on `treatment_ads` yields a coefficient of `r round(coef(model_colleague)["treatment_ads"], 4)` with a p-value of `r signif(summary(model_colleague)$coefficients["treatment_ads", "Pr(>|t|)"], 4)`. Since treatment is applied starting in week 1, there should be no causal effect on week 0 purchases. The fact that the coefficient is positive and highly significant indicates that users with more treatment exposures already had higher baseline spending. Therefore, the placebo test fails and suggests that the treatment assignment is correlated with pre-existing differences in purchasing behavior and that simple regressions of `week1` on `treatment_ads` could lead to biased estimates of the treatment effect.


## What has gone wrong?
Here's the tip off: the placebo test suggests that there is something wrong with our experiment (i.e. the randomization isn't working) or our data analysis. We suggest looking for a problem with the data analysis. Do you see something that might be spoiling the "randomness" of the treatment variable? (Hint: it should be present in the cross-tab that you wrote in the first part of this qusetion.) How can you improve your analysis to address this problem? Why does the placebo test turn out the way it does? What one thing needs to be done to analyze the data correctly? Please provide a brief explanation of why, not just what needs to be done. 

**Answer: ** From the cross-tab, we can see that users with more total homepage visits inherently tend to have more treatment exposures. This means that `treatment_ads` is mechanically correlated with `total_ads`, which is itself correlated with week 0 purchasing behavior. Users who have more homepage visits during the campaign week (and thusly higher `total_ads`) are more likely to have higher treatment exposures (`treatment_ads`) and also had a higher baseline spending in week 0. Therefore, regressing `week0` on `treatment_ads` picks up the pre-existing correlation despite there being no treatment applied yet. If we adjust for total visits by including `total_ads` as a covariate in the regression, the mechanical correlation between the treatment and baseline purchase behavior will be removed.

## Conduct proposed solution and re-evaluate placebo test 
Implement the procedure you propose from part 3, run the placebo test for the Week 0 data again, and report the results. (This placebo test should pass; if it does not, re-evaluate your strategy before wasting time proceeding.) How can you tell this this has fixed the problem? Is it possible, even though this test now passes, that there is still some other problem? 

```{r passing placebo model, include=TRUE}
# Creating regression that integrates `total_ads` as the covariate
model_passes_placebo <- lm(week0 ~ treatment_ads + total_ads, data = d)
summary(model_passes_placebo)
```

**Answer:** The coefficient on treatment_ads is `r round(coef(model_passes_placebo)["treatment_ads"], 4)` with an insignificant p-value of `r signif(summary(model_passes_placebo)$coefficients["treatment_ads","Pr(>|t|)"], 4)`. This coefficient is effectively zero (in addition to being statistically insignificant), which indicates that once we account for total homepage visits, there is no longer a confounding relationship between treatment exposures and baseline purchase behavior. While this specific issue has been remedied, it is still possible that there are other confounding variables or the possibility of nonlinear treatment effects.

## Estimate treatment effect with proposed solution
Now estimate the causal effect of each ad exposure on purchases during Week 1. You should use the same technique that passed the placebo test in part 4. Describe how, if at all, the treatment estimate that your model produces changes from the estimate that your colleague produced.  

```{r causal model, include=TRUE} 
model_causal <- lm(week1 ~ treatment_ads + total_ads, data = d)
summary(model_causal)
```

**Answer:** In this model, the coefficient on `treatment_ads` is `r round(coef(model_causal)["treatment_ads"], 4)` with a p-value of `r signif(summary(model_causal)$coefficients["treatment_ads", "Pr(>|t|)"], 4)`. This indicates that each additional treatment ad exposure increases purchases in week 1 by approximately `r round(coef(model_causal)["treatment_ads"], 4)` dollars on average, after controlling for total homepage visits. Compared to the colleague's model, the adjusted estimate of `treatment_ads` is smaller in magnitude since their model confounded treatment with total visits.

## Defend you method 
Upon seeing these results, the colleague who proposed the specification that did not pass the placeo test challenges your results -- they make the campaign look less successful! Write a short paragraph (i.e. 4-6 sentences) that argues for why your estimation strategy is better positioned to estimate a causal effect. 

**Answer: ** While the colleague's model suggested a larger effect of `treatment_ads`, it fails to account for the fact that users with more homepage visits naturally receive more treatment exposures and also tend to spend more even before the campaign begins. In contrast, our estimation strategy adjusts for total homepage visits (total_ads) which isolates the variation in treatment ads that is independent of visit frequency. Since our specification passed the placebo test on week 0 purchases, we have shown that pre-campaign spending is no longer correlated with the treatment variable. By controlling for total visits, our approach yields a less biased, more accurate estimate of the causal effect of each ad exposure on week 1 purchases. Therefore, while the adjusted effect appears smaller, it more faithfully represents the true causal impact of the advertising campaign.

## Intertemporal substitution?
One concern raised by David Reiley is that advertisements might just shift *when* people purchase something -- rather than increasing the total amount they purchase. Given the data that you have available to you, can you propose a method of evaluating this concern? Estimate the model that you propose, and describe your findings. 

```{r feature engineering, include=TRUE}
# Defining a cumulative revenue variable
d$cumulative_week_1_10 <- rowSums(d[, paste0("week", 1:10)])

```

```{r overall effect, include=TRUE}
# Create cumulative model
model_overall <- lm(cumulative_week_1_10 ~ treatment_ads + total_ads, data = d)
summary(model_overall)
```

**Answer:** The cumulative model yields a coefficient on treatment_ads of `r round(coef(model_overall)["treatment_ads"], 4)` with a p-value of `r signif(summary(model_overall)$coefficients["treatment_ads","Pr(>|t|)"], 4)`. This coefficient is small and statistically insignificant, indicating that additional treatment ads do not significantly increase total spending over the ten weeks following the campaign. Comparing these results to the week 1 model (`model_causal`), where `treatment_ads` had a statistically significant positive effect, suggests that the campaign mainly accelerates purchases into week 1 rather than generating additional spending overall. This can be interpreted as the ads shifting the timing of purchases rather than increasing total revenue.

## Weekly effects
If you look at purchases in each week -- one regression estimated for each outcome from week 1 through week 10 (that's 10 regression in a row) -- what is the relationship between treatment ads and purchases in each of those weeks. This is now ranging into exploring data with models -- how many have we run in this question alone!? -- so consider whether a plot might help make whatever relationship exists more clear. 

```{r models for each week, include=TRUE}
library(ggplot2)

# Running one regression per week and storing results
weeks <- paste0("week", 1:10)
coef_treat <- numeric(length(weeks))
se_treat <- numeric(length(weeks))

for(i in seq_along(weeks)) {
  model <- lm(as.formula(paste(weeks[i], "~ treatment_ads + total_ads")), data = d)
  coef_treat[i] <- coef(model)["treatment_ads"]
  se_treat[i] <- summary(model)$coefficients["treatment_ads", "Std. Error"]
}
```

```{r plot of weekly treatment effects, include=TRUE}
# Creating data frame for plotting
plot_data <- data.frame(
  week = 1:10,
  coef = coef_treat,
  lower = coef_treat - 1.96 * se_treat,
  upper = coef_treat + 1.96 * se_treat
)

# Plotting treatment effect over time
ggplot(plot_data, aes(x = week, y = coef, color = factor(week))) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  scale_x_continuous(breaks = 1:10) +
  scale_color_brewer(palette = "Spectral") +
  labs(x = "Week", 
       y = "Effect of Treatment Ads on Purchases", 
       color = "Week",
       title = "Weekly Effect of Treatment Ads on Purchases") +
  theme_minimal()
```

**Answer:** We create a plot of `treatment_ads` coefficient estimates and 95% confidence intervals per week, where confidence intervals straddling 0 indicated statistically insignificant estimates. From the plot, we can see that week 1 sees a positive and statistically significant coefficient, showing that treatment ads increase purchases during this week. However, all of weeks 2-10 have statistically insignificant coefficients near 0. Together, these results suggest that treatment ads mainly accelerate purchases into week 1, rather than increasing overall spending throughout weeks 1-10.

## Evaluating what is happening in the data
I. What might explain this pattern in your data. Stay curious when you're writing models! But, also be clear that we're fitting a **lot** of models and making up a theory/explanation after the fact. 

**Answer: ** The observed pattern of a positive effect in week 1, but near-zero effects in weeks 2–10, could indicate a timing-shift where ads accelerate purchases that would have otherwise occurred later. This suggests that users respond immediately to ads, concentrating revenue in the campaign week rather than increasing overall spending. Other factors such as seasonality or user heterogeneity could also contribute to these results, so further analysis would be needed to confirm the specific mechanism at play.

## Evaluate whether there are non-linear relationships
We started by making the assumption that there was a linear relationship between the treatment ads and purchases. What other types of relationships might exist? After you propose at least two additional non-linear relationships, write a model that estimates these, and write a test for whether these non-linear effects you've proposed produce models that fit the data better than the linear model.  

```{r testing for non-linear effects, include=TRUE} 
# Creating a quadratic model
model_quadratic <- lm(week1 ~ treatment_ads + I(treatment_ads^2) + total_ads, data = d)
quad_summary <- summary(model_quadratic)

# Creating logarithmic model
model_log <- lm(week1 ~ log(1 + treatment_ads) + total_ads, data = d)
log_summary <- summary(model_log)

quad_summary
log_summary

# Comparing linear vs. quadratic model
cat("ANOVA comparison for linear vs. quadratic model: \n")
anova(model_causal, model_quadratic)
cat("\n")
# Comparing linear vs. logarithmic model
cat("ANOVA comparison for linear vs. logarithmic model: \n")
anova(model_causal, model_log)

```

**Answer:** We attempt to see if a quadratic (which could model potential diminishing returns) or logarithmic model (which could show potential saturation) is a better for for the data. Using ANOVA to compare the linear model with the quadratic model, we see that the quadratic term is near 0 (`r round(coef(model_quadratic)["I(treatment_ads^2)"], 4)`) and statistically insignificant (p = `r signif(summary(model_quadratic)$coefficients["I(treatment_ads^2)", "Pr(>|t|)"], 4)`). Comparing the linear model to the logarithmic model, we see a statistically significant coefficient of `r round(coef(model_log)["log(1 + treatment_ads)"], 4)` with a p-value of `r signif(summary(model_log)$coefficients["log(1 + treatment_ads)","Pr(>|t|)"], 4)`. While this coefficient is statistically significant, comparing the RSS of the linear model to the log-transformed model shows no meaningful improvement (a difference of only `r round((sum(resid(model_log)^2) - sum(resid(model_causal)^2)) / sum(resid(model_causal)^2) * 100, 4)`% in RSS). This confirms that the log transformation does not provide a better fit than the original linear specification. Taken together, these results show both non-linear specifications fail to improve model fit over the original linear model. Therefore the effect of `treatment_ads` on Week 1 purchases is well approximated as linear and positive, conditional on total homepage visits. It appears that non-linearities such as diminishing returns or saturation are not evident in the data.
